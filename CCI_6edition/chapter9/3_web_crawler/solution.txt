I am assuming a web crawler starts with a URL. It takes the URL does a GET HTTP request, parses that data if there is any, extracts all the URLs, and repeats that process with those discovered URLs. So the web crawler finds the connected components of this network of documents.

Im going to start with a small case. Lets consider a small website, like a small eCommerce store with under 1000 pages. I would set up the web crawler to use a HashSet with the URL to determine if the page was already visited. If this was the case, then search and the memory usage would be O(N) where N is the number of pages.
A HashSet might not be ideal for bigger cases, and the hash algorithm might not be very effective. Either way lets consider a case where there is no O(1) lookup. First lets consider if we just store the visited URL either in a List or a Trie. Well in these cases, it would run in O(N^2) and O(NL) respectively. O(N^2) because for every new document you have to search your existing documents. O(NL) because for every document your search for existing documents would be L in the Trie where L is the length of the largest URL.
Another approach is to save the hash of the documents checksum and use that for lookups, but that would only be more beneficial if the documents changed based on time.

All of these cases deal with infinite loops, but now lets consider how the infinite loops would occur in the real world. When your crawler is scanning the world wide web, its scanning trillions of documents and a HashSet might not be ideal. To perform a lookup you could have a dedicated server cluster to determine if the URL has already been searched. So without the additional data structure to check if the page has been visited, loops would occur and they would occur on almost every site because sites usually have a reference to the homepage. You could circumvent this issue with the following. When first visiting a site, visit the homepage and the nav links first and make it a rule to not visit them again. Loops can still occur but at least your not circling around the start point. How to determine if something is a nav link is not so easy, in HTML you can look for the header or nav tags, but that is not always the case. You can do the same thing for the footer links.

Another approach might be to start from the base url and only check pages that have an equal or greater than number of slashes in the pathname. This would not work poorly on sites that have a hierarchical URL structures, but it would still cause loops.

Another strategy might be to have a finite sized HashSet with an accompanying finitely sized queue binded to it. It would work as such: You start filling up the queue such that it is not full or is not about to be full. When pages are checked to see if visited in this case, you would check the HashSet and see if they exist. If the page exists skip it and if it doesn't exist it check the page, add the page to the queue and add it to the HashSet. When the queue is full, you still check to see if the page exists in the HashSet. If it's in the HashSet skip it, otherwise add it to the HashSet, dequeue the queue and remove that element from the HashSet. With this architecture there would only be at most one hash/page in the queue and the HashSet, so there wouldn't be any duplicates.  Loops are still possible in this architecture but at least you won't visit pages that you visited recently.
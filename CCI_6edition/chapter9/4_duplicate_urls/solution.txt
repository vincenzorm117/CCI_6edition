QUESTION: You have 10 billion URLs. How do you detect the duplicate documents? In this case, assume "duplicate" means that the URLs are identical.

First are we dealing with just these 10 billion URLs or could there be more? Does this happen once, more than once or does this happen periodically? I am going to assume it only happens once. Second dublicate documents means the URLs are identical so I am interpretting this problem as: How do you detect duplicate URLs in a list of 10 billion URLs? Third how are these URLs received and how are the duplicates presented/delivered?

Lets consider the case where we get a file that has all 10 billion URLs, this file would have to be some gigabytes in length from at least about 40 gigabytes to maybe a petabyte depending on the statistics regadrding the URL lengths. If we use a array duplicate algorithm with a hashset we could blow through our RAM. I think this is the challenge we have to figure out in this problem, so how do we solve it?

Lets divide this up into different operations like a Map-Reduce operation. First lets break the list into N=10 evenly divided sets. Then we would send each list of 1,000,000 URLs each to one of N=10 computers to find the duplicates. This would not perform very well but can be scaled with the number of mapped computers N. We can extend this and say that we have N=10 and each processes lets say M=10,000 URLs at a time that is generated by another computer. This would run 10*10^9/10/10,000 = 100,000 times. Each 10,000 batch would find duplicates and report them to another computer. This is still expensive because if each batch has to be checked to all batches.

Instead or finding duplicates, we can focus on sorting the list and findind duplicates in the list. We can perform a Map-Reduce like I did in the previous paragraph. This would be a Merge-Sort across a number of computers. Lets say we have N=10 so that each computer sorts 10,000,000,000 / 10 = 1,000,000,000 URLs and then sends the list over to a final computer to merge the lists as they come in. This would still be expensive, but at least it would be easier to use with future lists. Let consider the case where each of the N=10 computers sorts M=10,000 URLs at a time. In this case, the O(N^2) performance is on 10,000 as oppose to 1,000,000,000 so the initial sorting is faster, but there will be more sortings: 10,000,000,000 / 10,000 / 10 = 100,000. Finally, the merging is O(N) which can happen on another computer while the other computers are sorting.

In these cases, the problem with the N=10 computers processing M=10,000 URLs at a time presents the issue of how the network connection affects performance. 10,000 URL may take up less than 1MB of memory, but a network/IO call is still expensive.

This problem is concerned with duplicates so maybe we can optimize are previous solution. Lets consider the batch example. Instead of merging lists maybe we can have duplicates list of size D and a non-duplicates list of size U. This would work by each list being sorted, but the merging would be different. When a new list of size L comes in, each URL is binary searched on the duplicates list first. If the URL is in the duplicates list the URL is ignored. If the URL is not in the duplicates list then we check the non-duplicates list with a binary search as well. If the URL is not in the non-duplicates list we add it to the non-duplicates list. If it is in the non-duplicates list we remove it from the non-duplicates list and add it to the duplicates list. These operations would run in O( L*max(U,D) ) because in the worst case every element in the new list has to be added to the current lists.

Out of all the approaches I think this last one will perform the best. I think that it will still take some time to run as it is a lot of data.
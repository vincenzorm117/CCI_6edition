QUESTION: Imagine a web server for a simplified search engine. This system has 100 machines to respond to search queries, which may then call out using processSearch(string query) to another cluster of machines to actually get the result. The machine which responds to a given query is chosen at random, so you cannot guarantee that the same machine will always respond to the same request. The method processSearch is very expensive. Design a caching mechanism for the most recent queries. Be sure to explain how you would update the cache when data changes.

What are people searching for here and who are they? My immediate thought is that I am designing Google search, but this might be a database. Im going to start with the database first and see where this takes me. Also how often and from where are these queries being performed? If this is a one time thing, I would consider caching everything. Since the question mentions updating, Im assuming that this is not a series of requests that happen once or a few times, but some to many times. It's difficult to gaige bandwidth in this scenario since it sounds more like a latency/bandwidth question, so I am going to assume that internet/intranet connection speeds is not a concern and is a constant amount. What I mean by this is that were not worried about someone in the desert with a 5MBit connection requesting a query, but I'll address this briefly in my response.

Lets assume there are 100 machines that can respond first as stated above, lets call these level 0 machines. Then there are 100 machines for each one of those 100 machines, lets call these level 1 machines. There are a total of 100 + 100*100 = 10,100 machines. My immediate thought is that the caching mechanism will just work like a HashMap, where I can do a look up quickly in O(1) time given the query string return the query response. Lets say there is a query A that is searched a lot, if it is searchecd more than 10,100 times, it will most likely be cached on all level 0 machines and returned quickly so there are no issues there, but if the content of the query is updated we would have to invalidate all the caches. How will this work? Each machine woud have to have an invalidate function that removes the hash from the HashMap on each machine that has it. invalidate would be called on all machines. This presents the problem that if the query QueryA is requested often then it could not be updated often as it would slow down traffic as processSearch would have to be called, so this reduces latency and could reduce throughput. Instead of removing the hash maybe we can just update it by having a function updateQuery(string query, Response document) where document is the query response. Then latency would not be affected assuming the query response is cached, but it may not necessarily affect throughput because some query responses might be bigger than others. Lets consider a case where a document is several gigabytes. If a level 0 computer is responding to it, it would attempt to cache the entry and for several gigabytes that would most likely not work well individually and as a whole as most conventional RAM memory is just a few gigaytes. Maybe for something like that we could off-load it to another computer that deals with large query responses. This machine would have to have a lot of RAM, use flash memory, and have a fast network adapter to have quick read/write and broadcast functionality. By doing that, you would avoid reducing availability because you would reduce the amount of busy nodes responding to queries with large responses. This does not solve the problem of somebody with a slow internet connection though. You could add some sort of compression in response transmissions, but I believe that is beyond the scope of this question. Lets consider the case where all query responses are at most 20MB, and there are thousands to millions of requests per second. The primary concern in this case is latency, and second would be throughput. The caching mechanism in this case would perform well when query response updates are minimal. One way to fix this might be to sort the machines such that they each handle different query types. This sorting would distribute the load, but may not necessarily be evenly distributed. Maybe a certain kind of query is requested more than others. That may decrease latency and throughput.

How would the caching work exactly? The caching would be a finite sized HashMap with an accompanying finitely sized queue binded to it. The key would be the query string and the value the query response. It would work as such: You start filling up the queue such that it is not full or is not about to be full. When queries are checked to see if already processed in this case, you would check the HashMap and see if they exist. If the query exists return it's value and if it doesn't exist run processSearch, add the query to the queue and add it to the HashMap with its query response in the HashMap. When the queue is full, you still check to see if the query exists in the HashMap. If it's in the HashMap return its response, otherwise add it to the HashMap, dequeue the queue and remove that query from the HashMap. With this architecture there would only be at most one query in the queue and the HashMap, so there wouldn't be any duplicates. The issue with this setup is that it takes up double the amount of memory. We could reduce that by having a data structure, lets call it a Node, that wraps the query. This Node will be used to build a doubly linked list that is stored in the HashMap. When a query is added to the HashMap we store as the query as the key and the value being the Node. The Node would have a reference to the next and previous Nodes in the doubly linked list, a reference to the query and its response. When a Node is added we wrap it with the Node mark it as the head and make it point ot he previous head. If the HashMap has not reached the designated limit size then we simply just add the element. Note that the first element added is the tail of the doubly linked list, this will be important when needing to dequeue when the HashMap has reached its designated size. So when the HashMap has reached its designated size the element is added as usually making it the new head, but the tail is removed. When the tail is removed the new tail is the next Node from the tail and the tail Node is removed from the HashMap. This architecture supports high latency and high throughput assuming non-giant query responses.